{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "779c1d7c",
   "metadata": {},
   "source": [
    "# ü§ñ Entra√Ænement & Comparaison de Mod√®les Profonds ‚Äì DCASE 2024\n",
    "\n",
    "Ce notebook automatise l'entra√Ænement et l'√©valuation de plusieurs architectures de deep learning pour l‚Äôanalyse des sons :\n",
    "\n",
    "- Autoencodeur dense (reconstruction)\n",
    "- Autoencodeur convolutionnel\n",
    "- CNN simple\n",
    "- CNN profond\n",
    "- LeNet modifi√©\n",
    "\n",
    "Les mod√®les sont compar√©s √† partir :\n",
    "- de leurs courbes de convergence (loss/accuracy)\n",
    "- de leur matrice de confusion\n",
    "- de leurs scores globaux sur un ensemble test√©\n",
    "\n",
    "üìÅ Les mod√®les sont export√©s (`.h5`) et pr√™ts √† √™tre int√©gr√©s √† l‚Äôinterface Streamlit.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3317b804",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìå Librairies standards\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import librosa\n",
    "import librosa.display\n",
    "from pathlib import Path\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# üìå Configuration globale\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# üìÇ Ajout dynamique du r√©pertoire principal au PYTHONPATH\n",
    "project_root = Path.cwd().parent  # DCASE2024_ASD_Project/\n",
    "sys.path.append(str(project_root))\n",
    "\n",
    "\n",
    "# üîß Projet\n",
    "from src import config, processing\n",
    "from src.utils.logger_utils import logger\n",
    "from src.models import baseline_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76b77a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üß± Param√®tres globaux\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 50\n",
    "RANDOM_STATE = 42\n",
    "IMG_SIZE = (64, 64)  # Pour les spectrogrammes en entr√©e CNN\n",
    "CHANNELS = 1         # Format (H, W, C)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca418ff",
   "metadata": {},
   "source": [
    "## üß© Bloc 1 ‚Äì Extraction des spectrogrammes √† partir des fichiers `.wav`\n",
    "\n",
    "Nous allons :\n",
    "- Parcourir tous les fichiers `.wav` du dossier `dev_data/`\n",
    "- Extraire des repr√©sentations 2D (MFCC, log-Mel)\n",
    "- G√©n√©rer un ensemble `X` (features 2D) et `y` (labels binaires)\n",
    "\n",
    "Format final attendu : `X.shape = (N, H, W, C)`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ecfc3244",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_wav_and_extract_features(path, feature_type=\"mel\", img_size=(64, 64)):\n",
    "    \"\"\"\n",
    "    Charge un fichier audio et extrait un spectrogramme 2D (Mel ou MFCC).\n",
    "\n",
    "    Args:\n",
    "        path (str or Path): Chemin vers le fichier .wav\n",
    "        feature_type (str): \"mel\" ou \"mfcc\"\n",
    "        img_size (tuple): Taille finale de l'image\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: image 2D (H, W)\n",
    "        int: label (0 = normal, 1 = anomaly)\n",
    "    \"\"\"\n",
    "    signal, sr = librosa.load(path, sr=config.SR)\n",
    "    if feature_type == \"mfcc\":\n",
    "        feat = librosa.feature.mfcc(y=signal, sr=sr, n_mfcc=config.N_MFCC)\n",
    "    elif feature_type == \"mel\":\n",
    "        mel = librosa.feature.melspectrogram(y=signal, sr=sr, n_mels=config.N_MELS)\n",
    "        feat = librosa.power_to_db(mel, ref=np.max)\n",
    "    else:\n",
    "        raise ValueError(\"feature_type must be 'mel' or 'mfcc'\")\n",
    "\n",
    "    # Resize (padding/truncation) to (H, W)\n",
    "    feat = librosa.util.fix_length(feat, size=img_size[1], axis=1)\n",
    "    feat = feat[:img_size[0], :]  # crop height if necessary\n",
    "\n",
    "    # Normalisation\n",
    "    feat = (feat - np.min(feat)) / (np.max(feat) - np.min(feat) + 1e-8)\n",
    "\n",
    "    # Get label\n",
    "    label = 1 if \"anomaly\" in path.name else 0\n",
    "\n",
    "    return feat, label\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d209cb6",
   "metadata": {},
   "source": [
    "## üì¶ Bloc 2 ‚Äì G√©n√©ration de l'ensemble complet (X, y)\n",
    "\n",
    "Nous allons maintenant :\n",
    "- Lire tous les fichiers `.wav`\n",
    "- Extraire une image spectrogramme 2D pour chacun\n",
    "- Cr√©er le jeu de donn√©es :\n",
    "  - `X` : images (H, W, C)\n",
    "  - `y` : 0 = normal, 1 = anomaly\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "889fa198",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# üìÇ Dossier contenant les fichiers .wav\n",
    "audio_dir = config.DCASE_DIR / \"dev_data\"\n",
    "all_audio_files = sorted(audio_dir.glob(\"**/*.wav\"))\n",
    "\n",
    "X_list = []\n",
    "y_list = []\n",
    "\n",
    "# üéõÔ∏è Type de spectrogramme √† utiliser\n",
    "FEATURE_TYPE = \"mel\"  # ou \"mfcc\"\n",
    "\n",
    "for path in tqdm(all_audio_files, desc=\"Extraction audio\"):\n",
    "    try:\n",
    "        spec, label = load_wav_and_extract_features(path, feature_type=FEATURE_TYPE, img_size=IMG_SIZE)\n",
    "        X_list.append(spec)\n",
    "        y_list.append(label)\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Erreur avec {path.name}: {e}\")\n",
    "\n",
    "# üìê Format final : (N, H, W, C)\n",
    "X = np.array(X_list)[..., np.newaxis]  # Ajoute canal unique (C=1)\n",
    "y = np.array(y_list)\n",
    "\n",
    "logger.info(f\"‚úÖ Dataset g√©n√©r√© : X shape = {X.shape}, y shape = {y.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "971b0a52",
   "metadata": {},
   "source": [
    "## üß™ Bloc 3 ‚Äì Split & Pr√©paration des donn√©es pour entra√Ænement\n",
    "\n",
    "Nous d√©coupons les donn√©es (80/20) de mani√®re stratifi√©e selon le label (normal/anomaly)  \n",
    "et pr√©parons les cibles pour les mod√®les de classification (encodage one-hot).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab4f78e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# ‚ö†Ô∏è S√©paration supervision / non supervision (pour AE)\n",
    "X_normal = X[y == 0]\n",
    "\n",
    "# ‚úÖ Split pour les mod√®les supervis√©s\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "# üî¢ Encodage one-hot pour classifieurs CNN\n",
    "y_train_cat = to_categorical(y_train, num_classes=2)\n",
    "y_test_cat = to_categorical(y_test, num_classes=2)\n",
    "\n",
    "logger.info(f\"‚úÖ Split termin√© : X_train={X_train.shape}, y_train={y_train_cat.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd566c21",
   "metadata": {},
   "source": [
    "## üß† Bloc 4 ‚Äì Entra√Ænement automatique des mod√®les profonds\n",
    "\n",
    "Nous entra√Ænons les architectures suivantes :\n",
    "- Autoencodeur dense (`autoencoder_model`)\n",
    "- Autoencodeur convolutionnel (`autoencoder`)\n",
    "- Classifieur CNN simple\n",
    "- Classifieur CNN profond\n",
    "- LeNet modifi√©\n",
    "\n",
    "üìÅ Tous les mod√®les sont sauvegard√©s pour usage ult√©rieur dans Streamlit.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdee2bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "models_dict = {}\n",
    "\n",
    "# üìÅ Dossier de sauvegarde\n",
    "save_dir = config.MODEL_DIR / \"deep\"\n",
    "save_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# üîÅ Liste des mod√®les √† entra√Æner\n",
    "architectures = {\n",
    "    \"AE_Dense\": (autoencoder_model(X_normal[0].shape), X_normal, X_normal),\n",
    "    \"AE_Conv\": (autoencoder(shape=(*IMG_SIZE, CHANNELS)), X_normal, X_normal),\n",
    "    \"CNN_Simple\": (cnn_simple_model((*IMG_SIZE, CHANNELS), 2), X_train, y_train_cat),\n",
    "    \"CNN_Deep\": (cnn_model((*IMG_SIZE, CHANNELS), 2), X_train, y_train_cat),\n",
    "    \"LeNet\": (LeNet_model((*IMG_SIZE, CHANNELS), 2), X_train, y_train_cat),\n",
    "}\n",
    "\n",
    "for name, (model, X_in, y_in) in architectures.items():\n",
    "    logger.info(f\"üöÄ Entra√Ænement du mod√®le : {name}\")\n",
    "\n",
    "    # Callbacks\n",
    "    ckpt_path = save_dir / f\"{name}.h5\"\n",
    "    checkpoint = ModelCheckpoint(ckpt_path, monitor=\"val_loss\", save_best_only=True, verbose=0)\n",
    "    earlystop = EarlyStopping(monitor=\"val_loss\", patience=5, restore_best_weights=True)\n",
    "\n",
    "    # Ajuster y_val en fonction AE ou classifier\n",
    "    if \"AE\" in name:\n",
    "        X_val = X_in[int(len(X_in)*0.8):]\n",
    "        y_val = X_val\n",
    "        X_train_sub = X_in[:int(len(X_in)*0.8)]\n",
    "        y_train_sub = X_train_sub\n",
    "    else:\n",
    "        X_train_sub, X_val, y_train_sub, y_val = train_test_split(X_in, y_in, test_size=0.2, stratify=y_in, random_state=RANDOM_STATE)\n",
    "\n",
    "    # Fit\n",
    "    history = model.fit(\n",
    "        X_train_sub, y_train_sub,\n",
    "        validation_data=(X_val, y_val),\n",
    "        epochs=EPOCHS,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        verbose=1,\n",
    "        callbacks=[checkpoint, earlystop]\n",
    "    )\n",
    "\n",
    "    models_dict[name] = model\n",
    "    plot_model_history(history)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d1d44ff",
   "metadata": {},
   "source": [
    "## üìä Bloc 5 ‚Äì √âvaluation des mod√®les de classification (CNNs)\n",
    "\n",
    "Ce bloc charge les mod√®les classifieurs entra√Æn√©s (CNN_Simple, CNN_Deep, LeNet)  \n",
    "et √©value leur performance sur l‚Äôensemble de test : `X_test`, `y_test_cat`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e2c88d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "# üîÅ Nom + fichier + label ground truth\n",
    "cnn_models = [\"CNN_Simple\", \"CNN_Deep\", \"LeNet\"]\n",
    "eval_results = []\n",
    "\n",
    "for name in cnn_models:\n",
    "    model_path = save_dir / f\"{name}.h5\"\n",
    "    model = tf.keras.models.load_model(model_path)\n",
    "\n",
    "    # üîç Pr√©diction\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_class = np.argmax(y_pred, axis=1)\n",
    "    y_true_class = np.argmax(y_test_cat, axis=1)\n",
    "\n",
    "    acc = accuracy_score(y_true_class, y_pred_class)\n",
    "    f1 = f1_score(y_true_class, y_pred_class)\n",
    "\n",
    "    eval_results.append({\n",
    "        \"Model\": name,\n",
    "        \"Accuracy\": acc,\n",
    "        \"F1 Score\": f1\n",
    "    })\n",
    "\n",
    "    # üìâ Rapport complet\n",
    "    print(f\"üîç Rapport pour : {name}\")\n",
    "    print(classification_report(y_true_class, y_pred_class, target_names=[\"Normal\", \"Anomaly\"]))\n",
    "    plot_confusion_matrix(y_true_class, y_pred_class, class_names=[\"Normal\", \"Anomaly\"], title=f\"{name} ‚Äì Confusion Matrix\")\n",
    "\n",
    "# üìä Tableau r√©sum√©\n",
    "df_eval = pd.DataFrame(eval_results).sort_values(\"F1 Score\", ascending=False).round(4)\n",
    "display(df_eval)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dadf1127",
   "metadata": {},
   "source": [
    "## üì§ Bloc 6 ‚Äì Export des r√©sultats des mod√®les CNN\n",
    "\n",
    "Les scores d'√©valuation (`Accuracy`, `F1 Score`) de tous les mod√®les classifieurs sont sauvegard√©s dans un fichier CSV `cnn_evaluation_summary.csv`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12219497",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìÅ Dossier d'export\n",
    "eval_path = config.PREDICTIONS_DIR / \"cnn_evaluation_summary.csv\"\n",
    "df_eval.to_csv(eval_path, index=False)\n",
    "logger.info(f\"‚úÖ R√©sultats export√©s dans : {eval_path}\")\n",
    "display(df_eval)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Deep Learning Env)",
   "language": "python",
   "name": "deep_learning_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
